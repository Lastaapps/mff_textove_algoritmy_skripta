#import "../definitions.typ": *

= Regular Expressions

== Introduction

Regular expressions are a powerful tool for defining and searching for patterns in text. This chapter covers the fundamental concepts of regular expressions, their relationship with finite automata, and algorithms for efficient pattern matching.

- *Input:* A text (haystack) $T$ and a regular expression $v$.
- *Output:* All positions in $T$ that mark the end of a substring (a needle) belonging to the language $L(v)$ generated by the regular expression.
- *Notation:*
  - $n = |T|$ is the length of the text.
  - $m$ is the "size" of the regular expression $v$, typically the number of characters from the alphabet appearing in it.

#figure(
  image("../figures/regex-nfa-title.png", width: 80%),
)

Language Generated by a Regular Expression
- $L(epsilon) = {epsilon}$
- $L(a) = {"a"}$
- $L(q_1 q_2) = {w_1 w_2 | w_1 in L(q_1), w_2 in L(q_2)}$
- $L(q_1 | q_2) = L(q_1) union L(q_2)$
- $L(q^*) = union_(i>=0) L(q^i)$ // (zero or more repetitions)

Other common operators like `+` (one or more repetitions) and `?` (zero or one occurrence) can be defined using the basic ones. Operator precedence is typically: Kleene star, concatenation, then union.

== Searching via Finite Automata

The standard approach to searching with regular expressions is to convert the expression into a finite automaton.

1. *Syntactic Analysis:* Parse the regular expression into a syntax tree.
2. *NFA Construction:* Convert the syntax tree into a Nondeterministic Finite Automaton (NFA).
3. *Simulation:* Simulate the NFA on the text to find matches.

Alternatively, the NFA can be converted to a Deterministic Finite Automaton (DFA) for faster, linear-time searching, at the cost of a potentially exponential increase in the number of states.

=== Thompson's Construction (Regex #sym.arrow NFA)

A famous method for building an NFA from a regular expression. The resulting NFA has a unique start and end state.

- *Base Case (character `a`):* A start state with a transition labeled 'a' to a final state.
- *Concatenation ($q_1 q_2$):* The final state of NFA($q_1$) is connected with an $epsilon$-transition to the start state of NFA($q_2$).
- *Union ($q_1 | q_2$):* A new start state has $epsilon$-transitions to the start states of both NFA($q_1$) and NFA($q_2$). A new final state is the target of $epsilon$-transitions from the final states of both.
- *Kleene Star ($q^*$):* A new start state has $epsilon$-transitions to the NFA($q$) start state and to a new final state. The NFA($q$) final state has $epsilon$-transitions back to its own start state and to the new final state.

#figure(
  grid(
    columns: (50%, 50%),
    rows: auto,
    align: center,
    image("../figures/regex-nfa-basic-operators.png", width: 60%),
    image("../figures/regex-nfa-star-operator.png", width: 80%),
  ),
)

#info_box(title: "Properties of Thompson's NFA")[
  - The construction is linear in the size of the regex: an NFA for a regex of size $m$ can be built in $O(m)$ time.
  - The resulting automaton has at most $2m$ states.
  - Each state has at most two outgoing $epsilon$-transitions or one non-$epsilon$-transition.
]

#info_box(title: "Lemmas on NFA(e) Construction and Size")[
  *Claim:* A regular expression $e$ of size $m$ (number of character occurrences from the alphabet) can be constructed to contain at most:
  - $2m$ parentheses
  - $m$ binary operators (union `|` and concatenation, e.g., in `(ab)`)
  - $2m$ occurrences of the Kleene star operator `*`

  *Corollary:* The total length of such a regular expression $e$, denoted as $|e|$, is at most $6m$.

  *Claim:* If NFA($e$) = $(V, E)$ is the NFA constructed from $e$ using Thompson's construction, then:
  - The number of states $|V|$ is at most $8m$.
  - The number of edges $|E|$ is at most $13m$.

  *Corollary:* An NFA for a regular expression $e$ of size $m$ can be constructed in $O(m)$ time.
]

=== NFA Simulation

To find matches of a regex $v$ in a text $T$, we *build an NFA for the regex $Sigma^* v$*. This allows a match to begin at any point in the text.

The simulation proceeds character by character through the text, keeping track of the set of currently active NFA states.

#code_box([
  #smallcaps([NFA-Search]) ($T$, NFA)
  ```
    // S is the set of active states
    S = epsilon_closure({start_state})
    for i from 0 to n-1:
      S_next = {}
      for state in S:
        if state has transition on T[i] to s_new:
          add s_new to S_next
      S = epsilon_closure(S_next)
      if S contains a final state:
        report match at i
  ```
])

- `epsilon_closure(states)`: Returns the set of all states reachable from the input `states` using only $epsilon$-transitions.
- *Complexity:* The NFA simulation is $O(m n)$, as for each of the $n$ text characters, we might process up to $m$ states.

=== DFA Conversion (Subset Construction)

To achieve faster search times, the NFA can be converted into an equivalent DFA.

- *Idea:* Each DFA state corresponds to a *subset* of NFA states.
- *Start State:* The start state of the DFA is the $epsilon$-closure of the NFA's start state.
- *Transitions:* From a DFA state $D$ (representing NFA states $S$), a transition on character `a` leads to a new DFA state $D'$ representing the $epsilon$-closure of all states reachable from $S$ on `a`.
- A DFA state is final if it contains any of the NFA's final states.

*Complexity:*
- *Search Time:* $O(n)$ after the DFA is built.
- *Construction Time & Space:* Can be exponential in the worst case ($O(2^m)$), which makes it impractical for very complex regular expressions. However, in many practical cases, the number of states remains manageable.

== Advanced Methods

=== Glushkov's NFA Construction

An alternative to Thompson's construction that creates an NFA without $epsilon$-transitions (except from a new start state). The construction is $O(m^2)$.

=== Wu-Manber Algorithm for Regular Expressions

This algorithm applies the bit-parallelism principle (from Shift-Or) to NFA simulation.

- A bit vector $s$ represents the active states of the NFA. $s[j] = 0$ means state $j$ is active.
- Character masks are precomputed, similar to Shift-Or.
- The key challenge is handling $epsilon$-transitions. This is solved by precomputing a table that maps a set of states to its $epsilon$-closure. The state vector is used as an index into this table.
- The update step becomes extremely fast: $s = T[ (s >> 1) | t[T[i]] ]$, where $T$ is the precomputed closure table.

=== Filtering Algorithms

For very long texts, it can be faster to first filter the text for potential match locations and then run a full verification on those locations only.

- *Watson's Algorithm:*
  1. Generate the set of all minimal-length prefixes of words in $L(v)$.
  2. Use a fast multi-pattern matcher (like Aho-Corasick) to find these prefixes.
  3. Run the NFA/DFA from each found prefix location to verify the full match.
- *GNU Grep Strategy:*
  1. Identify a set of "necessary factors"â€”substrings that must appear in any word of the language $L(v)$.
  2. Find these factors in the text.
  3. For each occurrence, try to extend the match in both directions to see if it corresponds to a full word in $L(v)$.

#example_box(title: "Example: Grep Strategy")[
  For the regex `(print|write)f.*(bug|error)`, the string $"f"$ is a necessary factor. The grep tool would first quickly scan for all occurrences of $"f"$ and only then start its more complex regex engine at those locations.
]

== Tasks

=== Task 1
Construct the NFA for the regular expression $(a|b)*a b$ using Thompson's construction.

=== Task 2
Why can the subset construction from an NFA to a DFA lead to an exponential number of states? Provide a small example or an intuition.

=== Task 3
What is the primary advantage of using a filtering algorithm like the one used in `grep` before running the full regex matching engine?

#pagebreak()

== Solutions

=== Solution 1
Let's build the NFA for $(a|b)*a b$ step-by-step.
1. *NFA for $a$ and $b$*: Two simple NFAs with a start and end state, connected by an edge labeled $a$ and $b$ respectively.
2. *NFA for $a|b$*: A new start state with $epsilon$-transitions to the start states of NFA(a) and NFA(b). A new end state which is the target of $epsilon$-transitions from the end states of NFA(a) and NFA(b).
3. *NFA for $(a|b)*$*: A new start state (which is also the final state) with an $epsilon$-transition to the start of NFA(a|b). The final state of NFA(a|b) has $epsilon$-transitions back to its start state and to the new final state.
4. *NFA for $(a|b)*a$*: Concatenate NFA for $(a|b)*$ with NFA for $a$. The final state of the first becomes non-final and gets an $epsilon$-transition to the start of the second.
5. *NFA for $(a|b)*a b$*: Concatenate the result from step 4 with NFA for $b$.

The final automaton will have a path for zero or more $a$'s or $b$'s, followed by a mandatory $a$, and then a mandatory $b$.

=== Solution 2
The subset construction creates a DFA state for every possible *subset* of NFA states. If an NFA has $m$ states, there are $2^m$ possible subsets of these states.

*Intuition:* Consider the regex $(a|b)(a|b)...(a|b)$ ($m$ times). The NFA for this is simple. However, after reading a string like $a b a...$, the NFA could be in many different states simultaneously. For a string of length $k < m$, the set of active NFA states can depend on the specific sequence of $a$'s and $b$'s. A different sequence can lead to a different subset of active NFA states. The DFA must have a unique state for each of these reachable subsets. A regex like $.*a.{m-1}$ (a pattern ending with 'a' with m-1 any characters before it) is a classic example that forces the DFA to remember the positions of the last $a$ seen in the last $m$ characters, leading to a large number of states.

=== Solution 3
The primary advantage is speed, especially for large files and complex patterns. Regex engines, particularly NFA-based ones, can be relatively slow ($O(m n)$). Many patterns contain simple, literal substrings (or "necessary factors") that must exist for a match to be possible.
1. *Fast Pre-filtering:* Searching for a simple, fixed string is extremely fast (e.g., using algorithms like Boyer-Moore or Aho-Corasick, which are often close to $O(n)$).
2. *Reducing Expensive Work:* By first identifying the locations of these mandatory substrings, the expensive, full regex engine only needs to be run on a few, small portions of the text. If the necessary factor is rare, this can eliminate over 99% of the text from consideration, leading to a massive performance improvement.

#pagebreak()
